{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNS+GX0pXVqa4s1sSGLdTEB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shahensha5624/Developing-Generative-AI-Applications-with-Python/blob/main/Voice_Assistant_with_OpenAI's_GPT_3_and_IBM_Watson.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Generative AI-Powered Meeting Assistant**\n",
        "\n",
        " Consider you're attending a business meeting where all conversations are being captured by an advanced AI application. This application not only transcribes the discussions with high accuracy but also provides a concise summary of the meeting, emphasizing the key points and decisions made.\n",
        "\n",
        "In our project, we'll use OpenAI's Whisper to transform speech into text. Next, we'll use IBM Watson's AI to summarize and find key points. We'll make an app with Hugging Face Gradio as the user interface."
      ],
      "metadata": {
        "id": "4WIzrWq1Sp2V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Preparing the environment**\n",
        "\n",
        "Let's start with setting up the environment by creating a Python virtual environment and installing the required libraries, using the following commands in the terminal:"
      ],
      "metadata": {
        "id": "Ah_wMqHzSttR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jayDOFkX8M_i",
        "outputId": "939aa0c1-9162-4196-a886-6926d12e4f35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/bin/python\n",
            "Python 3.10.12\n"
          ]
        }
      ],
      "source": [
        "!which python\n",
        "!python --version"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%env PYTHONPATH="
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G8fH64E49zv9",
        "outputId": "65e3b6f0-8221-41cd-9745-8f6a2b871f28"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: PYTHONPATH=\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install virtualenv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1W4bKhL892PZ",
        "outputId": "f58a8baa-301d-4fe5-f4e8-316136d2151f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: virtualenv in /usr/local/lib/python3.10/dist-packages (20.26.6)\n",
            "Requirement already satisfied: distlib<1,>=0.3.7 in /usr/local/lib/python3.10/dist-packages (from virtualenv) (0.3.8)\n",
            "Requirement already satisfied: filelock<4,>=3.12.2 in /usr/local/lib/python3.10/dist-packages (from virtualenv) (3.16.1)\n",
            "Requirement already satisfied: platformdirs<5,>=3.9.1 in /usr/local/lib/python3.10/dist-packages (from virtualenv) (4.3.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!virtualenv my_env # create a virtual environment my_env\n",
        "!source my_env/bin/activate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xNUUfZY--SZZ",
        "outputId": "fc845e72-69be-40b6-b923-bcada9ff365e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "created virtual environment CPython3.10.12.final.0-64 in 622ms\n",
            "  creator CPython3Posix(dest=/content/my_env, clear=False, no_vcs_ignore=False, global=False)\n",
            "  seeder FromAppData(download=False, pip=bundle, setuptools=bundle, wheel=bundle, via=copy, app_data_dir=/root/.local/share/virtualenv)\n",
            "    added seed packages: pip==24.2, setuptools==75.1.0, wheel==0.44.0\n",
            "  activators BashActivator,CShellActivator,FishActivator,NushellActivator,PowerShellActivator,PythonActivator\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# installing required libraries in my_env\n",
        "!pip install transformers==4.35.2 torch==2.1.1 gradio==4.44.0 langchain==0.0.343 ibm_watson_machine_learning==1.0.335 huggingface-hub==0.19.4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a9ah9e5M-byf",
        "outputId": "13ee385b-cb70-450f-b563-0345653a363e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers==4.35.2 in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: torch==2.1.1 in /usr/local/lib/python3.10/dist-packages (2.1.1)\n",
            "Requirement already satisfied: gradio==4.44.0 in /usr/local/lib/python3.10/dist-packages (4.44.0)\n",
            "Requirement already satisfied: langchain==0.0.343 in /usr/local/lib/python3.10/dist-packages (0.0.343)\n",
            "Requirement already satisfied: ibm_watson_machine_learning==1.0.335 in /usr/local/lib/python3.10/dist-packages (1.0.335)\n",
            "Requirement already satisfied: huggingface-hub==0.19.4 in /usr/local/lib/python3.10/dist-packages (0.19.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.35.2) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.35.2) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.35.2) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.35.2) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.35.2) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.35.2) (2.32.2)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers==4.35.2) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.35.2) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.35.2) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.1.1) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.1.1) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.1.1) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.1) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.1.1) (2024.6.1)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.1) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.1) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.1) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.1) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.1) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.1) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.1) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.1) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.1) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.1) (2.18.1)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.1) (12.1.105)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.1) (2.1.0)\n",
            "Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.10/dist-packages (from gradio==4.44.0) (23.2.1)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio==4.44.0) (3.7.1)\n",
            "Requirement already satisfied: fastapi<1.0 in /usr/local/lib/python3.10/dist-packages (from gradio==4.44.0) (0.115.0)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.10/dist-packages (from gradio==4.44.0) (0.4.0)\n",
            "Requirement already satisfied: gradio-client==1.3.0 in /usr/local/lib/python3.10/dist-packages (from gradio==4.44.0) (1.3.0)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from gradio==4.44.0) (0.27.2)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio==4.44.0) (6.4.5)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio==4.44.0) (2.1.5)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio==4.44.0) (3.7.1)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio==4.44.0) (3.10.7)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio==4.44.0) (1.5.3)\n",
            "Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio==4.44.0) (10.4.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio==4.44.0) (2.9.2)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (from gradio==4.44.0) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.9 in /usr/local/lib/python3.10/dist-packages (from gradio==4.44.0) (0.0.10)\n",
            "Requirement already satisfied: ruff>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from gradio==4.44.0) (0.6.8)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio==4.44.0) (2.10.0)\n",
            "Requirement already satisfied: tomlkit==0.12.0 in /usr/local/lib/python3.10/dist-packages (from gradio==4.44.0) (0.12.0)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio==4.44.0) (0.12.5)\n",
            "Requirement already satisfied: urllib3~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio==4.44.0) (2.2.3)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from gradio==4.44.0) (0.31.0)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.343) (2.0.35)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.343) (3.10.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.343) (4.0.3)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.343) (0.6.7)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.343) (1.33)\n",
            "Requirement already satisfied: langchain-core<0.1,>=0.0.7 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.343) (0.0.13)\n",
            "Requirement already satisfied: langsmith<0.1.0,>=0.0.63 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.343) (0.0.92)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.343) (8.5.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from ibm_watson_machine_learning==1.0.335) (2024.8.30)\n",
            "Requirement already satisfied: lomond in /usr/local/lib/python3.10/dist-packages (from ibm_watson_machine_learning==1.0.335) (0.3.3)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from ibm_watson_machine_learning==1.0.335) (0.9.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.10/dist-packages (from ibm_watson_machine_learning==1.0.335) (8.5.0)\n",
            "Requirement already satisfied: ibm-cos-sdk<2.14.0,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from ibm_watson_machine_learning==1.0.335) (2.13.6)\n",
            "Requirement already satisfied: websockets<13.0,>=10.0 in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.3.0->gradio==4.44.0) (12.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.1.1) (12.6.68)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.343) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.343) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.343) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.343) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.343) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.343) (1.11.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio==4.44.0) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio==4.44.0) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio==4.44.0) (1.2.2)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.0.343) (3.22.0)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.0.343) (0.9.0)\n",
            "Requirement already satisfied: starlette<0.39.0,>=0.37.2 in /usr/local/lib/python3.10/dist-packages (from fastapi<1.0->gradio==4.44.0) (0.38.6)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio==4.44.0) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio==4.44.0) (0.14.0)\n",
            "Requirement already satisfied: ibm-cos-sdk-core==2.13.6 in /usr/local/lib/python3.10/dist-packages (from ibm-cos-sdk<2.14.0,>=2.12.0->ibm_watson_machine_learning==1.0.335) (2.13.6)\n",
            "Requirement already satisfied: ibm-cos-sdk-s3transfer==2.13.6 in /usr/local/lib/python3.10/dist-packages (from ibm-cos-sdk<2.14.0,>=2.12.0->ibm_watson_machine_learning==1.0.335) (2.13.6)\n",
            "Requirement already satisfied: jmespath<=1.0.1,>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from ibm-cos-sdk<2.14.0,>=2.12.0->ibm_watson_machine_learning==1.0.335) (1.0.1)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from ibm-cos-sdk-core==2.13.6->ibm-cos-sdk<2.14.0,>=2.12.0->ibm_watson_machine_learning==1.0.335) (2.9.0.post0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain==0.0.343) (3.0.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio==4.44.0) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio==4.44.0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio==4.44.0) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio==4.44.0) (1.4.7)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio==4.44.0) (3.1.4)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio==4.44.0) (2024.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio==4.44.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio==4.44.0) (2.23.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.35.2) (3.3.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain==0.0.343) (3.1.1)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio==4.44.0) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio==4.44.0) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio==4.44.0) (13.8.1)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata->ibm_watson_machine_learning==1.0.335) (3.20.2)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from lomond->ibm_watson_machine_learning==1.0.335) (1.16.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.1.1) (1.3.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio==4.44.0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio==4.44.0) (2.18.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain==0.0.343) (1.0.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio==4.44.0) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt update\n",
        "!sudo apt install ffmpeg -y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mBjP_MyE-mNZ",
        "outputId": "d70dfa28-2b0e-4bae-842a-874c3af09ef0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m\r0% [Working]\u001b[0m\r            \rHit:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:3 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Ign:4 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:6 https://r2u.stat.illinois.edu/ubuntu jammy Release\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "54 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 54 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**you can use your own audio file after uploading it in your notebook**\n",
        "\n",
        "make sure the file path is correct"
      ],
      "metadata": {
        "id": "wLUXzaGjTBW7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#simple_speech2text.py   [Downloading the required audio file]\n",
        "\n",
        "import requests\n",
        "\n",
        "# URL of the audio file to be downloaded\n",
        "url = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-GPXX04C6EN/Testing%20speech%20to%20text.mp3\"\n",
        "\n",
        "# Send a GET request to the URL to download the file\n",
        "response = requests.get(url)\n",
        "\n",
        "# Define the local file path where the audio file will be saved\n",
        "audio_file_path = \"downloaded_audio.mp3\"\n",
        "\n",
        "# Check if the request was successful (status code 200)\n",
        "if response.status_code == 200:\n",
        "    # If successful, write the content to the specified local file path\n",
        "    with open(audio_file_path, \"wb\") as file:\n",
        "        file.write(response.content)\n",
        "    print(\"File downloaded successfully\")\n",
        "else:\n",
        "    # If the request failed, print an error message\n",
        "    print(\"Failed to download the file\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hLHezoi1__qR",
        "outputId": "bf28f681-dd39-4f0a-87db-71276dd22f0a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File downloaded successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade huggingface_hub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XD-m3zgzC0ES",
        "outputId": "87ac4eb8-f01f-4857-f8fe-6a08af702dc2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.19.4)\n",
            "Collecting huggingface_hub\n",
            "  Using cached huggingface_hub-0.25.1-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.32.2)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2024.8.30)\n",
            "Using cached huggingface_hub-0.25.1-py3-none-any.whl (436 kB)\n",
            "Installing collected packages: huggingface_hub\n",
            "  Attempting uninstall: huggingface_hub\n",
            "    Found existing installation: huggingface-hub 0.19.4\n",
            "    Uninstalling huggingface-hub-0.19.4:\n",
            "      Successfully uninstalled huggingface-hub-0.19.4\n",
            "Successfully installed huggingface_hub-0.25.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Speech-to-Text**"
      ],
      "metadata": {
        "id": "mWjSyxjjEyG1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import pipeline\n",
        "\n",
        "# Initialize the speech-to-text pipeline from Hugging Face Transformers\n",
        "# This uses the \"openai/whisper-tiny.en\" model for automatic speech recognition (ASR)\n",
        "# The `chunk_length_s` parameter specifies the chunk length in seconds for processing\n",
        "pipe = pipeline(\n",
        "  \"automatic-speech-recognition\",\n",
        "  model=\"openai/whisper-tiny.en\",\n",
        "  chunk_length_s=30,\n",
        ")\n",
        "\n",
        "# Define the path to the audio file that needs to be transcribed\n",
        "sample = 'downloaded_audio.mp3'\n",
        "\n",
        "# Perform speech recognition on the audio file\n",
        "# The `batch_size=8` parameter indicates how many chunks are processed at a time\n",
        "# The result is stored in `prediction` with the key \"text\" containing the transcribed text\n",
        "prediction = pipe(sample, batch_size=8)[\"text\"]\n",
        "\n",
        "# Print the transcribed text to the console\n",
        "print(prediction)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IRmV5UERBVsU",
        "outputId": "1ca927ca-aae8-45fd-a5ae-9c324661f2b1"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:90: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Hello, I want to speak fast because I want to test four speech-to-text applications. Today, whether it's sunny, with a slight breeze, making it perfect for outdoor activity, later I plan for a busy local part, maybe even a picnic. The test is designed to assess the accuracy and responsiveness of the speech-to-text feature. Thank you for participating in this test.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Creating audio transcription app**"
      ],
      "metadata": {
        "id": "oJALmfU6ERd7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#speech2text_app.py\n",
        "\n",
        "import torch\n",
        "from transformers import pipeline\n",
        "import gradio as gr\n",
        "\n",
        "# Function to transcribe audio using the OpenAI Whisper model\n",
        "def transcript_audio(audio_file):\n",
        "    # Initialize the speech recognition pipeline\n",
        "    pipe = pipeline(\n",
        "        \"automatic-speech-recognition\",\n",
        "        model=\"openai/whisper-tiny.en\",\n",
        "        chunk_length_s=30,\n",
        "    )\n",
        "    # Transcribe the audio file and return the result\n",
        "    result = pipe(audio_file, batch_size=8)[\"text\"]\n",
        "    return result\n",
        "\n",
        "# Set up Gradio interface\n",
        "audio_input = gr.Audio(sources=\"upload\", type=\"filepath\")  # Audio input\n",
        "output_text = gr.Textbox()  # Text output\n",
        "\n",
        "# Create the Gradio interface with the function, inputs, and outputs\n",
        "iface = gr.Interface(fn=transcript_audio,\n",
        "                     inputs=audio_input, outputs=output_text,\n",
        "                     title=\"Audio Transcription App\",\n",
        "                     description=\"Upload the audio file\")\n",
        "\n",
        "# Launch the Gradio app\n",
        "iface.launch(server_name=\"0.0.0.0\", server_port=7860)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 626
        },
        "id": "9LTCu-v8Bxpo",
        "outputId": "75c8d815-da5d-4283-f5c9-53baed22ff7d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://57d4e4be820ab2a899.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://57d4e4be820ab2a899.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**use your watsonx.AI APIkey**"
      ],
      "metadata": {
        "id": "RleL45BKTdzI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ibm_watson_machine_learning.foundation_models import Model\n",
        "from ibm_watson_machine_learning.foundation_models.extensions.langchain import WatsonxLLM\n",
        "from ibm_watson_machine_learning.metanames import GenTextParamsMetaNames as GenParams\n",
        "\n",
        "my_credentials = {\n",
        "    \"url\"    : \"https://us-south.ml.cloud.ibm.com\"\n",
        "    \"apikey\": \"<your_apikey>\"  # Replace with your actual API key\n",
        "}\n",
        "\n",
        "params = {\n",
        "        GenParams.MAX_NEW_TOKENS: 800, # The maximum number of tokens that the model can generate in a single run.\n",
        "        GenParams.TEMPERATURE: 0.1,   # A parameter that controls the randomness of the token generation. A lower value makes the generation more deterministic, while a higher value introduces more randomness.\n",
        "    }\n",
        "\n",
        "LLAMA2_model = Model(\n",
        "        model_id= 'meta-llama/llama-2-70b-chat',\n",
        "        credentials=my_credentials,\n",
        "        params=params,\n",
        "        project_id=\"skills-network\",\n",
        "        )\n",
        "\n",
        "llm = WatsonxLLM(LLAMA2_model)\n",
        "\n",
        "print(llm(\"How to read a book effectively?\"))"
      ],
      "metadata": {
        "id": "dn5rCHHgUrXw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Integrating LLM: Using Llama 2 in WatsonX as LLM**\n",
        "\n",
        "In this exercise, we'll set up a language model (LLM) instance, which could be IBM WatsonxLLM, HuggingFaceHub, or an OpenAI model. Then, we'll establish a prompt template. These templates are structured guides to generate prompts for language models, aiding in output organization.\n",
        "\n",
        "Next, we'll develop a transcription function that employs the OpenAI Whisper model to convert speech-to-text. This function takes an audio file uploaded through a Gradio app interface (preferably in .mp3 format). The transcribed text is then fed into an LLMChain, which integrates the text with the prompt template and forwards it to the chosen LLM. The final output from the LLM is then displayed in the Gradio app's output textbox."
      ],
      "metadata": {
        "id": "YsvDlJFBFyaa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "import gradio as gr\n",
        "\n",
        "#from langchain.llms import OpenAI\n",
        "from langchain.llms import HuggingFaceHub\n",
        "\n",
        "from transformers import pipeline\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "from ibm_watson_machine_learning.foundation_models import Model\n",
        "from ibm_watson_machine_learning.foundation_models.extensions.langchain import WatsonxLLM\n",
        "from ibm_watson_machine_learning.metanames import GenTextParamsMetaNames as GenParams\n",
        "\n",
        "# Updated credentials to include the apikey\n",
        "my_credentials = {\n",
        "    \"url\"    : \"https://us-south.ml.cloud.ibm.com\",\n",
        "    \"apikey\": \"<your_apikey>\"  # Replace with your actual API key\n",
        "}\n",
        "params = {\n",
        "        GenParams.MAX_NEW_TOKENS: 800, # The maximum number of tokens that the model can generate in a single run.\n",
        "        GenParams.TEMPERATURE: 0.1,   # A parameter that controls the randomness of the token generation. A lower value makes the generation more deterministic, while a higher value introduces more randomness.\n",
        "    }\n",
        "\n",
        "LLAMA2_model = Model(\n",
        "        model_id= 'meta-llama/llama-2-70b-chat',\n",
        "        credentials=my_credentials,\n",
        "        params=params,\n",
        "        project_id=\"skills-network\",\n",
        "        )\n",
        "\n",
        "llm = WatsonxLLM(LLAMA2_model)\n",
        "\n",
        "#######------------- Prompt Template-------------####\n",
        "\n",
        "temp = \"\"\"\n",
        "<s><<SYS>>\n",
        "List the key points with details from the context:\n",
        "[INST] The context : {context} [/INST]\n",
        "<</SYS>>\n",
        "\"\"\"\n",
        "\n",
        "pt = PromptTemplate(\n",
        "    input_variables=[\"context\"],\n",
        "    template= temp)\n",
        "\n",
        "prompt_to_LLAMA2 = LLMChain(llm=llm, prompt=pt)\n",
        "\n",
        "#######------------- Speech2text-------------####\n",
        "\n",
        "def transcript_audio(audio_file):\n",
        "    # Initialize the speech recognition pipeline\n",
        "    pipe = pipeline(\n",
        "        \"automatic-speech-recognition\",\n",
        "        model=\"openai/whisper-tiny.en\",\n",
        "        chunk_length_s=30,\n",
        "    )\n",
        "    # Transcribe the audio file and return the result\n",
        "    transcript_txt = pipe(audio_file, batch_size=8)[\"text\"]\n",
        "    result = prompt_to_LLAMA2.run(transcript_txt)\n",
        "\n",
        "    return result\n",
        "\n",
        "#######------------- Gradio-------------####\n",
        "\n",
        "audio_input = gr.Audio(sources=\"upload\", type=\"filepath\")\n",
        "output_text = gr.Textbox()\n",
        "\n",
        "iface = gr.Interface(fn= transcript_audio,\n",
        "                    inputs= audio_input, outputs= output_text,\n",
        "                    title= \"Audio Transcription App\",\n",
        "                    description= \"Upload the audio file\")\n",
        "\n",
        "iface.launch(server_name=\"0.0.0.0\", server_port=7860)"
      ],
      "metadata": {
        "id": "8NIQToezE-mr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Text generation with LLM** : I've created a Python script to generate text using a model from the Hugging Face Hub, learned about some key parameters that influence the model's output, and have a basic understanding of how to switch between different LLM models.\n",
        "\n",
        "**Speech-to-Text conversion** : Utilized OpenAI's Whisper technology to convert lecture recordings into text, accurately.\n",
        "\n",
        "**Content summarization** : Implemented IBM Watson's AI to effectively summarize the transcribed lectures and extract key points.\n",
        "\n",
        "**User interface development** : Created an intuitive and user-friendly interface using Hugging Face Gradio, ensuring ease of use for students and educators."
      ],
      "metadata": {
        "id": "RIYbX8_mR9U0"
      }
    }
  ]
}